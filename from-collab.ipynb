{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z8UsoQ09qdOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7cc82a8-bb62-40da-e2fb-ea47059839ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bigdl-dllib-spark3==0.14.0b20211107\n",
            "  Downloading https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl (93.9 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m67.4/93.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:15\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 473, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 76, in resolve\n",
            "    collected = self.factory.collect_root_requirements(root_reqs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 538, in collect_root_requirements\n",
            "    reqs = list(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 494, in _make_requirements_from_install_req\n",
            "    cand = self._make_base_candidate_from_link(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 231, in _make_base_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "                                       ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 303, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 314, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 527, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 598, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 170, in unpack_url\n",
            "    file = get_http_url(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 111, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/network/download.py\", line 148, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='master.dl.sourceforge.net', port=443): Read timed out.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install latest pre-release version of bigdl-dllib with spark3\n",
        "# Find the latest bigdl-dllib with spark3 from https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/ and intall it\n",
        "!pip install https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl\n",
        "\n",
        "exit() # restart the runtime to refresh installed pkg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade bigdl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsSR8m9gr2cl",
        "outputId": "b5d21a94-e24d-4643-f3e0-2f0cdf95ae36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bigdl\n",
            "  Downloading bigdl-2.4.0-py3-none-manylinux1_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting bigdl-orca==2.4.0 (from bigdl)\n",
            "  Downloading bigdl_orca-2.4.0-py3-none-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting bigdl-nano==2.4.0 (from bigdl)\n",
            "  Downloading bigdl_nano-2.4.0-py3-none-manylinux2010_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting bigdl-chronos==2.4.0 (from bigdl)\n",
            "  Downloading bigdl_chronos-2.4.0-py3-none-manylinux1_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting bigdl-friesian==2.4.0 (from bigdl)\n",
            "  Downloading bigdl_friesian-2.4.0-py3-none-manylinux1_x86_64.whl.metadata (955 bytes)\n",
            "Collecting bigdl-serving==2.4.0 (from bigdl)\n",
            "  Downloading bigdl_serving-2.4.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting pandas<=1.3.5,>=1.0.5 (from bigdl-chronos==2.4.0->bigdl)\n",
            "  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn<=1.0.2,>=0.22.0 (from bigdl-chronos==2.4.0->bigdl)\n",
            "  Downloading scikit-learn-1.0.2.tar.gz (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from bigdl.dllib.nncontext import *\n",
        "from bigdl.dllib.keras.layers import *\n",
        "from bigdl.dllib.keras.models import *\n",
        "#import bigdl.dllib.keras.Sequential\n",
        "from bigdl.dllib.nnframes import *\n",
        "from bigdl.dllib.nn.criterion import *"
      ],
      "metadata": {
        "id": "lrke_MEziJEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = init_nncontext(cluster_mode=\"local\") # run in local mode\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ7plA6wiPrK",
        "outputId": "d95c01fe-eedb-4a9e-f903-a434e0e0ea09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current pyspark location is : /usr/local/lib/python3.11/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.11/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.2-0.14.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
            "Successfully got a SparkContext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTAR ARCHIVO KAGGEL"
      ],
      "metadata": {
        "id": "f5psEJHejrK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/kaggle\"\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_4YkjQmjtMH",
        "outputId": "dc2a17e1-1a8d-44c2-f35f-4745425c9f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/drive/MyDrive/kaggle/kaggle.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Descargar el dataset\n",
        "dataset_name = 'paultimothymooney/chest-xray-pneumonia'\n",
        "output_dir = 'chest_xray_pneumonia'\n",
        "\n",
        "# Descargar y descomprimir\n",
        "api.dataset_download_files(dataset_name, path=output_dir, unzip=True)\n",
        "\n",
        "print(f\"Dataset descargado y descomprimido en la carpeta: {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2ruXHRmhBz",
        "outputId": "0d6668eb-bd76-47e2-d2b1-14d9095ee183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "Dataset descargado y descomprimido en la carpeta: chest_xray_pneumonia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = spark.read.format('csv').options(inferSchema=True, header=True).load('caiis-dogfood-day-2020/train.csv')"
      ],
      "metadata": {
        "id": "QgdOR5eqkbQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CÓDIGO CHATGPT\n"
      ],
      "metadata": {
        "id": "KbeaX_-cqrSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "from pyspark.sql import SparkSession\n",
        "from bigdl.dllib.nncontext import init_nncontext\n",
        "from bigdl.dllib.keras.models import Sequential\n",
        "from bigdl.dllib.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from bigdl.dllib.optim.optimizer import Adam\n",
        "from pyspark.ml.image import ImageSchema\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "10odzi8JrTrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PneumoniaDetection\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = init_nncontext(\"PneumoniaDetection\")\n"
      ],
      "metadata": {
        "id": "vxkIO3PEtH0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para preprocesar datos\n",
        "def preprocess_data(base_dir):\n",
        "    \"\"\"\n",
        "    Carga y preprocesa imágenes desde los directorios de entrenamiento y prueba.\n",
        "    \"\"\"\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "    # Leer imágenes como DataFrames de Spark\n",
        "    train_df = ImageSchema.readImages(train_dir)\n",
        "    test_df = ImageSchema.readImages(test_dir)\n",
        "\n",
        "    # Extraer rutas y etiquetas\n",
        "    train_paths = train_df.select(\"image.origin\").rdd.map(lambda row: row[0]).collect()\n",
        "    train_labels = [1 if \"PNEUMONIA\" in path else 0 for path in train_paths]\n",
        "\n",
        "    test_paths = test_df.select(\"image.origin\").rdd.map(lambda row: row[0]).collect()\n",
        "    test_labels = [1 if \"PNEUMONIA\" in path else 0 for path in test_paths]\n",
        "\n",
        "    return train_paths, train_labels, test_paths, test_labels\n"
      ],
      "metadata": {
        "id": "U2M26UBdtNL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construir el modelo con BigDL\n",
        "def build_model():\n",
        "    \"\"\"\n",
        "    Define un modelo CNN usando BigDL Keras API.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(32, 3, 3, activation=\"relu\", input_shape=(3, 64, 64)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "618OPnkptQIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir imágenes a tensores y normalizar\n",
        "def process_images(paths):\n",
        "    \"\"\"\n",
        "    Convierte imágenes en tensores normalizados.\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    images = []\n",
        "    for path in paths:\n",
        "        img = Image.open(path).resize((64, 64))\n",
        "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
        "        images.append(img_array.transpose((2, 0, 1)))  # Reordenar a (canales, alto, ancho)\n",
        "    return np.array(images)\n"
      ],
      "metadata": {
        "id": "cL1bZx46tSgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio base del dataset\n",
        "base_dir = \"chest_xray_pneumonia\"\n",
        "\n",
        "# Preprocesar datos\n",
        "train_paths, train_labels, test_paths, test_labels = preprocess_data(base_dir)\n",
        "\n",
        "# Procesar imágenes y etiquetas\n",
        "train_images = process_images(train_paths)\n",
        "test_images = process_images(test_paths)\n",
        "\n",
        "# Convertir etiquetas a formato compatible\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "# Construir modelo\n",
        "model = build_model()\n",
        "\n",
        "# Configurar optimizador\n",
        "model.compile(optimizer=Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Entrenar modelo\n",
        "model.fit(train_images, train_labels, batch_size=32, nb_epoch=10, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluar modelo\n",
        "evaluation = model.evaluate(test_images, test_labels)\n",
        "print(f\"Evaluación del modelo: {evaluation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "a3C0CqkGtVuQ",
        "outputId": "965ab2f8-d6b6-49b7-b865-123795e9dc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'_ImageSchema' object has no attribute 'readImages'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5ad9a977504b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Preprocesar datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Procesar imágenes y etiquetas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d5e5a17fb14b>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Leer imágenes como DataFrames de Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSchema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSchema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_ImageSchema' object has no attribute 'readImages'"
          ]
        }
      ]
    }
  ]
}